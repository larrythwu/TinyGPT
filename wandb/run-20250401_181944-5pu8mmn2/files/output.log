step 21800: train loss 0.5199, val loss 0.5767
/home/ubuntu/DORC-GPT/train.py:323: UserWarning: Converting a tensor with requires_grad=True to a scalar may lead to unexpected behavior.
Consider using tensor.detach() first. (Triggered internally at /pytorch/aten/src/ATen/native/Scalar.cpp:22.)
  lossf = loss.item() * gradient_accumulation_steps
iter 21800: loss 0.4735, time 12251.39ms, mfu -100.00%
iter 21810: loss 0.5127, time 65.85ms, mfu 51.12%
iter 21820: loss 0.5056, time 65.90ms, mfu 51.12%
iter 21830: loss 0.4830, time 65.70ms, mfu 51.13%
iter 21840: loss 0.5012, time 65.95ms, mfu 51.12%
iter 21850: loss 0.4923, time 66.09ms, mfu 51.10%
iter 21860: loss 0.4787, time 65.93ms, mfu 51.10%
iter 21870: loss 0.6079, time 66.38ms, mfu 51.06%
iter 21880: loss 0.5002, time 66.26ms, mfu 51.04%
iter 21890: loss 0.4940, time 65.75ms, mfu 51.05%
step 21900: train loss 0.5146, val loss 0.5800
iter 21900: loss 0.5148, time 8647.01ms, mfu 45.99%
iter 21910: loss 0.4853, time 66.21ms, mfu 46.47%
iter 21920: loss 0.5742, time 66.15ms, mfu 46.91%
iter 21930: loss 0.4876, time 66.64ms, mfu 47.27%
iter 21940: loss 0.4702, time 66.35ms, mfu 47.62%
iter 21950: loss 0.5049, time 66.34ms, mfu 47.93%
iter 21960: loss 0.5367, time 66.67ms, mfu 48.19%
iter 21970: loss 0.4349, time 66.21ms, mfu 48.45%
iter 21980: loss 0.4916, time 66.26ms, mfu 48.69%
iter 21990: loss 0.5290, time 66.16ms, mfu 48.91%
step 22000: train loss 0.4961, val loss 0.5767
iter 22000: loss 0.5038, time 8672.19ms, mfu 44.06%
iter 22010: loss 0.4803, time 66.37ms, mfu 44.72%
iter 22020: loss 0.5593, time 66.22ms, mfu 45.33%
iter 22030: loss 0.4833, time 66.29ms, mfu 45.88%
iter 22040: loss 0.5009, time 66.32ms, mfu 46.37%
iter 22050: loss 0.4988, time 66.50ms, mfu 46.79%
iter 22060: loss 0.5263, time 66.58ms, mfu 47.17%
iter 22070: loss 0.4971, time 66.59ms, mfu 47.51%
iter 22080: loss 0.5586, time 66.71ms, mfu 47.80%
iter 22090: loss 0.4779, time 66.93ms, mfu 48.05%
step 22100: train loss 0.5057, val loss 0.5787
iter 22100: loss 0.5185, time 8682.66ms, mfu 43.29%
iter 22110: loss 0.4463, time 66.90ms, mfu 43.99%
iter 22120: loss 0.4350, time 66.84ms, mfu 44.63%
iter 22130: loss 0.5159, time 66.86ms, mfu 45.20%
iter 22140: loss 0.5609, time 66.68ms, mfu 45.73%
iter 22150: loss 0.5548, time 66.72ms, mfu 46.20%
iter 22160: loss 0.4408, time 66.75ms, mfu 46.62%
iter 22170: loss 0.5206, time 66.58ms, mfu 47.02%
iter 22180: loss 0.5639, time 66.19ms, mfu 47.40%
iter 22190: loss 0.5275, time 66.59ms, mfu 47.72%
step 22200: train loss 0.5201, val loss 0.5732
iter 22200: loss 0.5061, time 8694.08ms, mfu 42.98%
iter 22210: loss 0.5034, time 66.53ms, mfu 43.75%
iter 22220: loss 0.5383, time 66.56ms, mfu 44.43%
iter 22230: loss 0.5406, time 66.56ms, mfu 45.04%
iter 22240: loss 0.4788, time 66.85ms, mfu 45.57%
iter 22250: loss 0.5713, time 66.69ms, mfu 46.07%
iter 22260: loss 0.4964, time 66.74ms, mfu 46.50%
iter 22270: loss 0.5586, time 66.85ms, mfu 46.89%
iter 22280: loss 0.6079, time 66.88ms, mfu 47.23%
iter 22290: loss 0.5173, time 66.93ms, mfu 47.54%
step 22300: train loss 0.5168, val loss 0.5824
iter 22300: loss 0.5474, time 8699.88ms, mfu 42.82%
iter 22310: loss 0.4986, time 66.99ms, mfu 43.57%
iter 22320: loss 0.5181, time 67.00ms, mfu 44.24%
iter 22330: loss 0.5447, time 66.85ms, mfu 44.85%
iter 22340: loss 0.5590, time 66.87ms, mfu 45.40%
iter 22350: loss 0.5156, time 66.89ms, mfu 45.89%
iter 22360: loss 0.4810, time 66.91ms, mfu 46.33%
iter 22370: loss 0.4800, time 66.88ms, mfu 46.73%
iter 22380: loss 0.4571, time 66.90ms, mfu 47.09%
iter 22390: loss 0.5112, time 66.92ms, mfu 47.41%
step 22400: train loss 0.5222, val loss 0.5817
iter 22400: loss 0.4706, time 8702.45ms, mfu 42.71%
iter 22410: loss 0.4538, time 67.03ms, mfu 43.46%
iter 22420: loss 0.4554, time 66.83ms, mfu 44.15%
iter 22430: loss 0.4692, time 66.95ms, mfu 44.77%
iter 22440: loss 0.4814, time 66.89ms, mfu 45.32%
iter 22450: loss 0.5251, time 67.03ms, mfu 45.81%
iter 22460: loss 0.5507, time 66.87ms, mfu 46.27%
iter 22470: loss 0.5426, time 67.04ms, mfu 46.66%
iter 22480: loss 0.5071, time 66.94ms, mfu 47.02%
iter 22490: loss 0.4334, time 66.91ms, mfu 47.35%
step 22500: train loss 0.5024, val loss 0.5812
iter 22500: loss 0.4980, time 8713.70ms, mfu 42.66%
iter 22510: loss 0.5284, time 66.84ms, mfu 43.43%
iter 22520: loss 0.5383, time 66.86ms, mfu 44.12%
iter 22530: loss 0.6470, time 66.89ms, mfu 44.74%
iter 22540: loss 0.4948, time 66.89ms, mfu 45.30%
iter 22550: loss 0.4680, time 66.88ms, mfu 45.80%
iter 22560: loss 0.5309, time 66.86ms, mfu 46.26%
iter 22570: loss 0.5231, time 66.58ms, mfu 46.69%
iter 22580: loss 0.5243, time 66.62ms, mfu 47.07%
iter 22590: loss 0.5017, time 66.74ms, mfu 47.41%
step 22600: train loss 0.4903, val loss 0.5812
iter 22600: loss 0.4599, time 8721.05ms, mfu 42.71%
iter 22610: loss 0.5231, time 66.67ms, mfu 43.49%
iter 22620: loss 0.5166, time 66.84ms, mfu 44.17%
iter 22630: loss 0.4698, time 66.53ms, mfu 44.82%
iter 22640: loss 0.4647, time 66.47ms, mfu 45.40%
iter 22650: loss 0.5069, time 66.71ms, mfu 45.91%
iter 22660: loss 0.4869, time 66.76ms, mfu 46.36%
iter 22670: loss 0.5527, time 66.69ms, mfu 46.77%
iter 22680: loss 0.4754, time 66.76ms, mfu 47.14%
iter 22690: loss 0.5322, time 67.02ms, mfu 47.45%
step 22700: train loss 0.5079, val loss 0.5807
iter 22700: loss 0.5039, time 8722.58ms, mfu 42.74%
iter 22710: loss 0.5483, time 66.61ms, mfu 43.52%
iter 22720: loss 0.5538, time 66.73ms, mfu 44.21%
iter 22730: loss 0.5900, time 66.58ms, mfu 44.85%
iter 22740: loss 0.5067, time 66.72ms, mfu 45.41%
iter 22750: loss 0.5009, time 66.79ms, mfu 45.91%
iter 22760: loss 0.4833, time 66.78ms, mfu 46.36%
iter 22770: loss 0.5323, time 66.79ms, mfu 46.76%
iter 22780: loss 0.5291, time 67.00ms, mfu 47.11%
iter 22790: loss 0.5229, time 66.82ms, mfu 47.44%
step 22800: train loss 0.5150, val loss 0.5777
iter 22800: loss 0.4997, time 8725.19ms, mfu 42.73%
iter 22810: loss 0.4889, time 66.80ms, mfu 43.50%
iter 22820: loss 0.5516, time 66.89ms, mfu 44.18%
iter 22830: loss 0.6547, time 66.83ms, mfu 44.80%
iter 22840: loss 0.5110, time 66.83ms, mfu 45.36%
iter 22850: loss 0.5288, time 66.90ms, mfu 45.85%
iter 22860: loss 0.5252, time 66.83ms, mfu 46.31%
iter 22870: loss 0.5063, time 66.98ms, mfu 46.70%
iter 22880: loss 0.5090, time 66.88ms, mfu 47.07%
iter 22890: loss 0.5734, time 67.17ms, mfu 47.37%
step 22900: train loss 0.5182, val loss 0.5791
iter 22900: loss 0.5262, time 8727.47ms, mfu 42.67%
iter 22910: loss 0.4804, time 66.88ms, mfu 43.44%
iter 22920: loss 0.4420, time 66.76ms, mfu 44.14%
iter 22930: loss 0.5252, time 66.68ms, mfu 44.77%
iter 22940: loss 0.5361, time 66.97ms, mfu 45.32%
iter 22950: loss 0.5050, time 66.86ms, mfu 45.82%
iter 22960: loss 0.4650, time 66.95ms, mfu 46.27%
iter 22970: loss 0.4718, time 66.87ms, mfu 46.68%
iter 22980: loss 0.5795, time 66.72ms, mfu 47.06%
iter 22990: loss 0.5442, time 66.69ms, mfu 47.40%
step 23000: train loss 0.5184, val loss 0.5877
iter 23000: loss 0.4428, time 8730.86ms, mfu 42.70%
iter 23010: loss 0.5341, time 66.99ms, mfu 43.45%
iter 23020: loss 0.5131, time 66.87ms, mfu 44.14%
iter 23030: loss 0.4609, time 66.61ms, mfu 44.78%
iter 23040: loss 0.5227, time 66.74ms, mfu 45.35%
iter 23050: loss 0.5597, time 66.76ms, mfu 45.86%
iter 23060: loss 0.4590, time 66.70ms, mfu 46.32%
iter 23070: loss 0.4560, time 66.75ms, mfu 46.73%
iter 23080: loss 0.5359, time 66.68ms, mfu 47.10%
iter 23090: loss 0.5027, time 66.69ms, mfu 47.44%
step 23100: train loss 0.5061, val loss 0.5870
iter 23100: loss 0.5621, time 8730.15ms, mfu 42.74%
iter 23110: loss 0.4769, time 66.94ms, mfu 43.49%
iter 23120: loss 0.4353, time 66.87ms, mfu 44.18%
iter 23130: loss 0.5339, time 66.97ms, mfu 44.79%
iter 23140: loss 0.5416, time 66.86ms, mfu 45.34%
iter 23150: loss 0.5069, time 67.06ms, mfu 45.83%
iter 23160: loss 0.4754, time 66.88ms, mfu 46.28%
iter 23170: loss 0.5020, time 66.66ms, mfu 46.70%
iter 23180: loss 0.4786, time 66.83ms, mfu 47.07%
iter 23190: loss 0.5021, time 66.64ms, mfu 47.41%
step 23200: train loss 0.4973, val loss 0.5832
iter 23200: loss 0.5286, time 8731.17ms, mfu 42.71%
iter 23210: loss 0.5079, time 66.82ms, mfu 43.48%
iter 23220: loss 0.5174, time 66.75ms, mfu 44.17%
iter 23230: loss 0.4880, time 66.72ms, mfu 44.80%
iter 23240: loss 0.5235, time 66.47ms, mfu 45.39%
iter 23250: loss 0.5149, time 66.80ms, mfu 45.89%
iter 23260: loss 0.4910, time 66.69ms, mfu 46.35%
iter 23270: loss 0.5367, time 66.93ms, mfu 46.74%
iter 23280: loss 0.5383, time 66.87ms, mfu 47.10%
iter 23290: loss 0.5058, time 66.78ms, mfu 47.43%
step 23300: train loss 0.5008, val loss 0.5841
iter 23300: loss 0.5072, time 8735.35ms, mfu 42.73%
iter 23310: loss 0.4996, time 66.94ms, mfu 43.48%
iter 23320: loss 0.5760, time 66.63ms, mfu 44.19%
iter 23330: loss 0.5533, time 66.84ms, mfu 44.81%
iter 23340: loss 0.4874, time 66.99ms, mfu 45.35%
iter 23350: loss 0.5040, time 66.87ms, mfu 45.85%
iter 23360: loss 0.5398, time 66.89ms, mfu 46.30%
iter 23370: loss 0.5310, time 67.12ms, mfu 46.68%
iter 23380: loss 0.5222, time 66.70ms, mfu 47.06%
iter 23390: loss 0.5039, time 66.68ms, mfu 47.40%
step 23400: train loss 0.5141, val loss 0.5849
iter 23400: loss 0.5498, time 8728.63ms, mfu 42.70%
iter 23410: loss 0.5299, time 66.89ms, mfu 43.47%
iter 23420: loss 0.5083, time 66.85ms, mfu 44.16%
iter 23430: loss 0.4953, time 66.81ms, mfu 44.78%
iter 23440: loss 0.4829, time 66.74ms, mfu 45.34%
iter 23450: loss 0.4999, time 66.67ms, mfu 45.86%
iter 23460: loss 0.5324, time 66.50ms, mfu 46.34%
iter 23470: loss 0.5491, time 66.51ms, mfu 46.76%
iter 23480: loss 0.4781, time 66.80ms, mfu 47.13%
iter 23490: loss 0.5158, time 66.72ms, mfu 47.46%
Traceback (most recent call last):
  File "/home/ubuntu/DORC-GPT/train.py", line 264, in <module>
    losses = estimate_loss()
  File "/home/ubuntu/.local/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
  File "/home/ubuntu/DORC-GPT/train.py", line 225, in estimate_loss
    losses[k] = loss.item()
KeyboardInterrupt
