step 0: train loss 10.9985, val loss 10.9971
/home/ubuntu/DORC-GPT/train.py:323: UserWarning: Converting a tensor with requires_grad=True to a scalar may lead to unexpected behavior.
Consider using tensor.detach() first. (Triggered internally at /pytorch/aten/src/ATen/native/Scalar.cpp:22.)
  lossf = loss.item() * gradient_accumulation_steps
iter 0: loss 10.9894, time 14856.46ms, mfu -100.00%
iter 10: loss 10.1543, time 2554.62ms, mfu 52.71%
iter 20: loss 9.3595, time 2558.21ms, mfu 52.70%
iter 30: loss 9.0045, time 2561.33ms, mfu 52.69%
iter 40: loss 8.7313, time 2565.91ms, mfu 52.67%
iter 50: loss 7.9805, time 2566.83ms, mfu 52.65%
iter 60: loss 7.5747, time 2568.78ms, mfu 52.63%
iter 70: loss 7.3767, time 2568.32ms, mfu 52.61%
iter 80: loss 7.0006, time 2570.26ms, mfu 52.59%
iter 90: loss 6.5920, time 2567.15ms, mfu 52.57%
step 100: train loss 6.2349, val loss 6.2216
saving checkpoint to out-wiki_zh
iter 100: loss 6.2168, time 13244.80ms, mfu 48.33%
iter 110: loss 5.7193, time 2567.58ms, mfu 48.74%
iter 120: loss 5.5754, time 2568.79ms, mfu 49.11%
iter 130: loss 5.1734, time 2568.33ms, mfu 49.44%
iter 140: loss 4.8214, time 2568.72ms, mfu 49.74%
iter 150: loss 4.5537, time 2569.35ms, mfu 50.01%
iter 160: loss 4.3032, time 2566.78ms, mfu 50.25%
iter 170: loss 4.1105, time 2568.29ms, mfu 50.47%
iter 180: loss 3.8741, time 2569.08ms, mfu 50.67%
iter 190: loss 4.0472, time 2567.48ms, mfu 50.84%
step 200: train loss 3.8714, val loss 3.8588
saving checkpoint to out-wiki_zh
iter 200: loss 3.8287, time 14586.37ms, mfu 46.68%
iter 210: loss 3.9301, time 2569.12ms, mfu 47.26%
iter 220: loss 3.7155, time 2568.37ms, mfu 47.77%
iter 230: loss 3.7477, time 2568.69ms, mfu 48.24%
iter 240: loss 3.7527, time 2569.17ms, mfu 48.66%
Traceback (most recent call last):
  File "/home/ubuntu/DORC-GPT/train.py", line 300, in <module>
    logits, loss = model(X, Y)
  File "/home/ubuntu/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ubuntu/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ubuntu/.local/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py", line 658, in _fn
    return fn(*args, **kwargs)
  File "/home/ubuntu/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ubuntu/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ubuntu/DORC-GPT/model.py", line 170, in forward
    def forward(self, idx, targets=None):
  File "/home/ubuntu/.local/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py", line 849, in _fn
    return fn(*args, **kwargs)
  File "/home/ubuntu/.local/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py", line 1207, in forward
    return compiled_fn(full_args)
  File "/home/ubuntu/.local/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 317, in runtime_wrapper
    all_outs = call_func_at_runtime_with_args(
  File "/home/ubuntu/.local/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/utils.py", line 126, in call_func_at_runtime_with_args
    out = normalize_as_list(f(args))
  File "/home/ubuntu/.local/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/utils.py", line 100, in g
    return f(*args)
  File "/home/ubuntu/.local/lib/python3.10/site-packages/torch/autograd/function.py", line 575, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/home/ubuntu/.local/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 1979, in forward
    fw_outs = call_func_at_runtime_with_args(
  File "/home/ubuntu/.local/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/utils.py", line 126, in call_func_at_runtime_with_args
    out = normalize_as_list(f(args))
  File "/home/ubuntu/.local/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 497, in wrapper
    return compiled_fn(runtime_args)
  File "/home/ubuntu/.local/lib/python3.10/site-packages/torch/_inductor/output_code.py", line 558, in __call__
    return self.current_callable(inputs)
  File "/home/ubuntu/.local/lib/python3.10/site-packages/torch/_inductor/utils.py", line 2435, in run
    return model(new_inputs)
  File "/tmp/torchinductor_ubuntu/mu/cmui3g4dl4ezm6bbmvt6pw37ysinmubp6u56k7yqblphuwpulpmq.py", line 1784, in call
    triton_per_fused__to_copy_add_native_layer_norm_native_layer_norm_backward_12.run(buf145, buf104, buf119, buf129, buf144, primals_37, buf149, buf151, buf339, 12288, 768, stream=stream0)
  File "/home/ubuntu/.local/lib/python3.10/site-packages/torch/_inductor/runtime/triton_heuristics.py", line 1045, in run
    return launcher(
  File "<string>", line 5, in launcher
  File "/home/ubuntu/.local/lib/python3.10/site-packages/triton/backends/nvidia/driver.py", line 529, in __call__
    self.launch(gridX, gridY, gridZ, stream, function, self.launch_cooperative_grid, global_scratch, *args)
KeyboardInterrupt
Exception ignored in atexit callback: <function _start_and_connect_service.<locals>.teardown_atexit at 0x70f252d0f490>
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.10/site-packages/wandb/sdk/lib/service_connection.py", line 94, in teardown_atexit
    conn.teardown(hooks.exit_code)
  File "/home/ubuntu/.local/lib/python3.10/site-packages/wandb/sdk/lib/service_connection.py", line 226, in teardown
    self._router.join()
  File "/home/ubuntu/.local/lib/python3.10/site-packages/wandb/sdk/interface/router.py", line 75, in join
    self._thread.join()
  File "/usr/lib/python3.10/threading.py", line 1096, in join
    self._wait_for_tstate_lock()
  File "/usr/lib/python3.10/threading.py", line 1116, in _wait_for_tstate_lock
    if lock.acquire(block, timeout):
KeyboardInterrupt:
